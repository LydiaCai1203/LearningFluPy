## REVIEW OF RESUME（阿菜2019年自制面经）

[某个大佬的面试心得](https://aleiwu.com/post/interview-experience/)

------------------
### 1. 换位思考
    我的简历上描述最多的就是爬虫的部分了，因为爬虫系统其实才是这个系统的核心，事实上我想找的更加偏向于后端业务逻辑系统的开发，所以我才放了后面的两个项目。但是面试官第一眼看到的就是爬虫的东西，所以一开始上来就问爬虫。
    
    到现在为止我一共经历了三场面试，三场面试无一例外都是更多的问一些和爬虫相关的东西多一些，然后是python的基础知识的掌握程度、对Django框架的了解、一些基础的后端编程知识。还有的问题就是，数据库各个表之间的关系，高并发的实现，爬虫的连接部分。

------------------
### 2. 我的想法
    首先我是一个应届毕业生，面试官问我的肯定都是一些基础的知识。我已经有一个在应届生里面比较能拿的出手的项目经验了，其实我只要把项目中遇到的难点以及基础知识解决了，我觉得就没有问题了。

------------------
### 3. 先自我介绍一下吧
    姓名，应届，工作经历，目前所负责过的项目。之前面试的时候我都是说到猫头鹰的时候就开始深入往下介绍了，由于猫头鹰的核心其实是爬虫系统，所以我这一次决定改变一下介绍的方式，我要把我简历上做过的三个项目全都说一遍，再开始进行项目介绍。

------------------
### 4. 你遇到过哪几种反爬的网站机制

#### 1. 通过设置request里的headers里的User-Agent来限制访问，有的网站会限制User-Agent访问白名单，只有在正常范围内的User-Agent，才能访问这个网站。

#### 2. 通过设置request里的headers里的Cookies来限制访问。
    1. 有的网站第一次访问的时候，返回的html里面内嵌这一js代码，这段js的代码的功能就是会生成一端随机的字符串，然后重新访问原网址，第二次访问是携带动态生成的cookies的，才能访问成功。比如银保监。
    
    2. 有的网站第一次访问的时候，会在返回的response的headers里面返回一个cookie1。然后在response的body里面某个标签里面隐藏一个cookie2, 接下来的访问需要携带这两个cookies才能返回正确的请求体。比如github.
    
    3. 有的网站需要登陆以后才能访问其它页面，模拟登录成功以后才能拿到cookies,携带这些cookies访问才能得到正确的相应体。厉害一点的网站，会在登录的时候发送的POST请求体中的用户名和密码加上密，这样我们在模拟登陆的时候就要花费点功夫了。
    
    4. 其实通过cookies限制访问的破解方法都是大同小异的，找到请求的顺序，能读懂js脚本，就可以解决大部分的反扒网站。但是想中央人民银行这样的，连js脚本都是加密过的，就很难做了。

#### 3. 通过ban掉频繁访问的ip来限制访问
    1. 比如微信搜狗，访问频繁以后就出现数字验证码。
    2. 比如深交所，访问频繁了以后就会限制访问，但是过一段时间就会恢复ip访问。
    3. 这种情况优先考虑建立和维护自己的ip代理池进行访问。
######（这里埋下了一个坑，面试官一定会问ip代理池是怎么做的）

#### 4. 验证码来限制访问
    1. 比如京东，比如微信搜狗，比如bilibili，比如12306
    2. 借助第三方的打码平台

------------------
### 4. 说一下你的ip代理池是怎么维护的
    1. 所有的IP代理来源是西刺免费代理，所有的爬取到的代理都放在Redis中。每个IP在刚抓取到的时候都会给到100分的初始分数。
    2. 会有一个Schedule定时去检查Redis中代理的可用性。刚爬取到的IP代理都存在redis的一个List1中。每个检测失败的IP都扣5分，如果还有剩余的分数，就存回到List1中。如果失败了，就从List中删除。如果检验通过了就存入另一个新的List2中。
    3. 其实还可以实现一个或者多个代理接口。

    题外话：
    王叔曾经提出一个方法，使用阿里云上面的虚拟手机，然后让android的同学写一个程序，每个手机都有一个自己的IP，把这台手机变成一个代理。其实我觉得蛮好的，反正后来也不知道什么原因就没有这样实现了。


------------------
### 4. 说一下是怎么使用celery把任务扔给爬虫的。
    1. 